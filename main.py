import streamlit as st
import os
import re
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory

# ---------- å­—ä¸²æ¸…ç†å‡½æ•¸ ----------
def clean_text(text):
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9\s,.?!:ï¼šï¼›ã€()\[\]ã€Œã€ã€ã€ã€‚ï¼Œï¼ï¼Ÿ\n]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ---------- åˆ†é¡é¸å–® ----------
category_map = {
    "äººäº‹è²¡å‹™": "hr_finance",
    "è³‡è¨Š": "it"
}
category = st.sidebar.selectbox("ğŸ“ è«‹é¸æ“‡åˆ†é¡", list(category_map.keys()))
category_key = category_map[category]
pdf_dir = os.path.join("pdfFiles", category_key)
vector_dir = os.path.join("vectorDB", category_key)
os.makedirs(pdf_dir, exist_ok=True)
os.makedirs(vector_dir, exist_ok=True)

# ---------- åˆå§‹åŒ–ç‹€æ…‹ ----------
st.title("ğŸ“š Local Chatbot System (Traditonal Chinese)")

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'qa_chain' not in st.session_state:
    st.session_state.qa_chain = None
if 'loaded_category' not in st.session_state:
    st.session_state.loaded_category = None

# ---------- Prompt èˆ‡è¨˜æ†¶ ----------
chat_template = """ä½ æ˜¯ä¸€å€‹è¦ªåˆ‡ä¸”çŸ¥è­˜è±å¯Œçš„ AI åŠ©ç†ï¼Œåªèƒ½æ ¹æ“šä¸‹æ–¹ context å›ç­”å•é¡Œï¼š
è«‹ç”¨å°ç£ç¹é«”ä¸­æ–‡å›ç­”ï¼Œèªæ°£è‡ªç„¶ã€å®Œæ•´ã€çµ¦éæŠ€è¡“å“¡å·¥çœ‹å¾—æ‡‚ã€‚

Context: {context}
History: {history}
User: {question}
AI:"""

prompt = PromptTemplate(
    input_variables=["history", "context", "question"],
    template=chat_template
)

memory = ConversationBufferWindowMemory(
    memory_key="history",
    return_messages=True,
    input_key="question",
    k=3
)

# ---------- è¼‰å…¥æ—¢æœ‰åˆ†é¡å‘é‡è³‡æ–™åº« ----------
if st.session_state.qa_chain is None or st.session_state.loaded_category != category_key:
    if os.path.exists(vector_dir):
        embedder = OllamaEmbeddings(model="nomic-embed-text")
        vectordb = Chroma(
            embedding_function=embedder,
            persist_directory=vector_dir
        )
        retriever = vectordb.as_retriever()
        llm = Ollama(model="gemma3:4b", base_url="http://localhost:11434")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            chain_type="stuff",
            chain_type_kwargs={"prompt": prompt, "memory": memory},
            verbose=False
        )
        st.session_state.qa_chain = qa_chain
        st.session_state.loaded_category = category_key
        st.success(f"âœ… å·²è¼‰å…¥ã€Œ{category}ã€åˆ†é¡çš„å‘é‡è³‡æ–™åº«")

# ---------- ä¸Šå‚³ PDF ä¸¦å»ºç«‹åˆ†é¡å‘é‡åº« ----------
uploaded_files = st.file_uploader("ğŸ“„ ä¸Šå‚³ PDF æ–‡ä»¶", type="pdf", accept_multiple_files=True)

if uploaded_files:
    all_docs = []

    for uploaded_file in uploaded_files:
        file_path = os.path.join(pdf_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        loader = PyPDFLoader(file_path)
        docs = loader.load()

        for doc in docs:
            doc.page_content = clean_text(doc.page_content)

        all_docs.extend(docs)

    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    split_docs = splitter.split_documents(all_docs)

    embedder = OllamaEmbeddings(model="nomic-embed-text")
    vectordb = Chroma.from_documents(
        split_docs,
        embedding=embedder,
        persist_directory=vector_dir
    )
    vectordb.persist()
    retriever = vectordb.as_retriever()

    llm = Ollama(model="gemma3:4b", base_url="http://localhost:11434")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt, "memory": memory},
        verbose=False
    )
    st.session_state.qa_chain = qa_chain
    st.session_state.loaded_category = category_key
    st.success(f"âœ…ã€Œ{category}ã€åˆ†é¡çš„å‘é‡è³‡æ–™åº«å·²æ›´æ–°")

# ---------- èŠå¤©è¼¸å…¥ ----------
question = st.chat_input("ğŸ’¬ è«‹è¼¸å…¥å•é¡Œ")
if question:
    if st.session_state.qa_chain:
        with st.spinner("AI æ€è€ƒä¸­..."):
            response = st.session_state.qa_chain.run(question)
    else:
        response = "âš ï¸ è«‹å…ˆä¸Šå‚³æ–‡ä»¶æˆ–é¸æ“‡å·²æœ‰åˆ†é¡"

    st.session_state.chat_history.append({"role": "user", "message": question})
    st.session_state.chat_history.append({"role": "assistant", "message": response})

# ---------- é¡¯ç¤ºå°è©±ç´€éŒ„ ----------
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["message"])
